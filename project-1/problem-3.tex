\documentclass[00-main.tex]{subfiles}
\begin{document}

\section*{Problem Three}

\subsection*{a.}
$\mathbf{a}$ and $\mathbf{b}$ are column vectors. Let $\mathbf{v} \neq 0$. Each row of $\mathbf{uv}^T$ is a multiple of $\mathbf{v}^T$ so the row space of $\mathbf{uv}^T$ has dimension 1. This implies that the rank of the matrix is 1.

\subsection*{b.}

\begin{align}
\nonumber
I = AA^{-1} = (I-\mathbf{uv}^T)(I+\gamma \mathbf{uv}^T) &= I + \gamma \mathbf{uv}^T - \mathbf{uv}^T - \mathbf{uv}^T \gamma \mathbf{uv}^T \\
\implies
( \gamma - 1) \mathbf{uv}^T + \gamma \mathbf{u} ( \mathbf{v}^T \mathbf{u}) \mathbf{v}^T &= 
( \gamma - 1) \mathbf{uv}^T + ( \mathbf{v}^T \mathbf{u}) \gamma \mathbf{u} \mathbf{v}^T
= 0
\end{align}

Since both $\gamma$ and the inner product of $\mathbf{u}$ and $\mathbf{v}$, $\mathbf{u}^T \mathbf{v}$, are scalars. Dividing by $\mathbf{uv}^T$ and solving for $\gamma$ yields

\begin{align}
\nonumber
( \gamma - 1) + ( \mathbf{v}^T \mathbf{u}) \gamma &= 0 \\
\implies
\gamma = \frac{1}{1+ \mathbf{v}^T \mathbf{u}}
\end{align}

\subsection*{c.}
As $\mathbf{v}^T \mathbf{u}$ approaches one, $\gamma$ approaches infinity. In that case, the matrix $A$ is singular and thus not invertible.

\subsection*{d.}
$\kappa_2(A) = ||A||_2 ||A^{-1}||_2 = ||I-\mathbf{uv}^T||_2 ||I+\gamma \mathbf{uv}^T||_2 $

An upper bound for $\kappa_2(A)$ can be found by using the triangle inequality.

\begin{align*}
\kappa_2(A) & \leq (||I||_2 + ||\mathbf{uv}^T||_2) (||I||_2+||\gamma \mathbf{uv}^T||_2) \\
&= (1 + ||\mathbf{uv}^T||_2) (1 + |\gamma| ||\mathbf{uv}^T||_2) \\
&= 1 + |\gamma| ||\mathbf{uv}^T||_2 + ||\mathbf{uv}^T||_2 + |\gamma| ||\mathbf{uv}^T||_2 ||\mathbf{uv}^T||_2 \\
& \leq 1 + |\gamma| ||\mathbf{u}||_2 ||\mathbf{v}^T||_2 + ||\mathbf{u}||_2 ||\mathbf{v}^T||_2  + |\gamma| ||\mathbf{u}||_2 ||\mathbf{v}^T||_2 ||\mathbf{u}||_2 ||\mathbf{v}^T||_2  \\
&= 1 + ||\mathbf{u}||_2 ||\mathbf{v}||_2 (1 + \gamma(1 + ||\mathbf{u}||_2 ||\mathbf{v}||_2 )) \\
\implies
\kappa_2(A) & \leq 1 + ||\mathbf{u}||_2 ||\mathbf{v}||_2 (1 + \gamma(1 + ||\mathbf{u}||_2 ||\mathbf{v}||_2 ))
\end{align*}

\subsection*{e.}
Let $B = I-\alpha E^{k,l}$. The matrix $\alpha E^{k,l}$ can be written as $\alpha \mathbf{ab}^T$, where $\mathbf{a}$ and $b$ are column vectors and $\mathbf{a}(k) = 1$ and zero otherwise, and $\mathbf{b}(l) = 1$ and zero otherwise. The result from \textbf{b.} gives

\begin{equation}
B^{-1} = I + \lambda E^{k,l}
\end{equation}

Solving $I = BB^{-1}$ yields $\lambda$:

\begin{align*}
I = BB^{-1} &= (I-\alpha E^{k,l}) (I + \lambda E^{k,l}) \\
&= I + \lambda E^{k,l} - \alpha E^{k,l} - (\alpha E^{k,l})(\lambda E^{k,l}) \\
&= I + E^{k,l}(\lambda - \alpha) - \alpha \lambda \delta_{kl} E^{k,l} \\
\implies
\alpha \lambda \delta_{kl} &= \lambda - \alpha \\
\implies
\lambda &= \frac{\alpha}{1 - \delta_{kl} \alpha}  
\end{align*}

$\delta_{kl}$ is the Kronecker delta function defined by

\begin{equation}
\delta_{kl}  = 
\left\{ \begin{array}{lll}
	0 & {\rm for} ~k \neq l \\
	1 & {\rm for} ~k = l   
\end{array} \right.
\end{equation}

The inverse of $B$ can thus be written as

\begin{equation}
B^{-1}  = 
\left\{ \begin{array}{lll}
	I + \alpha E^{k,l} & {\rm for} ~k \neq l \\
	I + \frac{\alpha}{1 - \alpha} E^{k,l} & {\rm for} ~k = l   
\end{array} \right.
\end{equation}

\subsection*{f.}
$\alpha$, $k$ and $l$ must be determined such that 
\begin{equation}
B^{(1)} = A^{(0)}B^{(0)} = (I - \alpha E^{k,l}) B^{(0)}
\end{equation}
has a zero-element in position $p,q$. Then, $k$ must be equal to $p$; 

\begin{equation}
k=p
\end{equation}

The element in position $p,q$ must obey

\begin{equation}
b^{(1)}_{pq} = b^{(0)}_{pq} - \alpha b^{(0)}_{lq} = 0
\end{equation}
Which implies that
\begin{equation}
\alpha = \frac{b^{(0)}_{pq}}{b^{(0)}_{lq}} %, l \neq p
\end{equation}

$l$ must be chosen such that $b_{lq} \neq 0$. With $l<p$ one can eliminate every element under the diagonal in $B$ using basic matrix operations. The matrix

\begin{equation}
B^{(r+1)} = A^{(r)}A^{(r-1)} ... A^{(0)}B^{(0)}
\end{equation}
has zero-elements below row $p$ in column $p$. $A^{(k)}$ will then take on the form $I - \alpha E^{k,p}$ where $\alpha = \frac{b^{(0)}_{pk}}{b^{(0)}_{pp}}$ and $k=p+1, ..., n$

\subsection{g.}
The matrix $A^{(r)}A^{(r-1)} ... A^{(0)}B^{(0)}$ can be expressed as

\begin{equation}
\label{3:prod}
A = \prod\limits_{k=p+1}^n ( I - \frac{b^{(0)}_{pk}}{b^{(0)}_{pp}} ) 
\end{equation}

Since $k \neq p$ and every element has different values for $k$, and since $E^{k,p} \cdot E^{l,p}$ equals a zero-matrix for $k \neq l$, \cref{3:prod} simplifies to 

\begin{equation}
\label{prod}
A = I - \sum \limits_{k=p+1}^n ( \frac{b^{(0)}_{pk}}{b^{(0)}_{pp}} ) 
\end{equation}

The inverse of every element in $A$ is known, so the inverse of $A$ becomes

\begin{align*}
A^{-1} &= (A^{(0)})^{-1} (A^{(1)})^{-1} ... (A^{(r)})^{-1} \\
&= \prod\limits_{k=p+1}^n \left( A^k \right)^{-1} \\
&= \prod\limits_{k=p+1}^n \left( I - \frac{b^{(0)}_{pk}}{b^{(0)}_{pp}} E^{k,p} \right) \\
&= I + \sum \limits_{k=p+1}^n \frac{b^{(0)}_{pk}}{b^{(0)}_{pp}} E^{k,p} 
\end{align*}

\subsection*{h.}
There does indeed exist a matrix $C^{k,l}$ that interchanges two rows. It's like the identity matrix, but the elements $(k,k)$ and $(l,l)$ are interchanged with $(k,l)$ and $(l,k)$, respectively.

\subsection{i.}
fda


\subsection*{j.}
If $A = I - \mathbf{uu}^T = I$ then $A$ is orthogonal.
\begin{align}
\nonumber
A^T A &= (I - \mathbf{uu}^T )^T (I - \mathbf{uu}^T) \\
\nonumber
(\mathbf{uu}^T)^T &= \mathbf{uu}^T; I^T = I \\
\implies
\nonumber
A^T A &= (I - \mathbf{uu}^T ) (I - \mathbf{uu}^T) = (I - \mathbf{uu}^T)^2 \\
\nonumber 
&= I - 2 \mathbf{uu}^T + ( \mathbf{uu}^T)^2 \\
&= I + \mathbf{uu}^T (\mathbf{u}^T\mathbf{u} - 2)
\label{3:i}
\end{align}
\cref{3:i} implies two possible values, namely $\mathbf{u} = 0$ in the trivial case, or $\mathbf{u}^T\mathbf{u} = 2$.
$A$ is thus orthogonal if the absolute value of  $\mathbf{u}$ is equal to $\sqrt{2}$.




%\bibliosub
\end{document}